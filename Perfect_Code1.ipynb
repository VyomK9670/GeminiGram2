{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c0b37-9422-417a-b5dc-d68606e4e34f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time\n",
    "import logging\n",
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from collections import Counter\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2bb66d-f29a-432f-9ed2-55c5bafb1d59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('news_scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load English language model for NLP\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    logger.error(\"Spacy model 'en_core_web_sm' not found. Please install it first.\")\n",
    "    logger.info(\"Run: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "class NewsScraper:\n",
    "    \"\"\"A robust news scraper that collects articles from multiple RSS feeds and websites.\"\"\"\n",
    "####################################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'TE': 'Trailers'\n",
    "        }\n",
    "        \n",
    "        # Configure requests session\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.session.max_redirects = 5\n",
    "        self.timeout = 15\n",
    "        \n",
    "        # News sources configuration\n",
    "        self.news_sources = {\n",
    "            'Hindustan Times': {\n",
    "                'rss': 'https://www.hindustantimes.com/feeds/rss/latest-news/rssfeed.xml',\n",
    "                'web': 'https://www.hindustantimes.com/latest-news',\n",
    "                'language': 'en',\n",
    "                'country': 'India'\n",
    "            },\n",
    "            'The Hindu': {\n",
    "                'rss': 'https://www.thehindu.com/feeder/default.rss',\n",
    "                'web': 'https://www.thehindu.com/latest-news/',\n",
    "                'language': 'en',\n",
    "                'country': 'India'\n",
    "            },\n",
    "            'Indian Express': {\n",
    "                'rss': 'https://indianexpress.com/feed/',\n",
    "                'web': 'https://indianexpress.com/latest-news/',\n",
    "                'language': 'en',\n",
    "                'country': 'India'\n",
    "            },\n",
    "            'BBC': {\n",
    "                'rss': 'http://feeds.bbci.co.uk/news/rss.xml',\n",
    "                'web': 'https://www.bbc.com/news',\n",
    "                'language': 'en',\n",
    "                'country': 'UK'\n",
    "            },\n",
    "            'CNN': {\n",
    "                'rss': 'http://rss.cnn.com/rss/cnn_latest.rss',\n",
    "                'web': 'https://edition.cnn.com/',\n",
    "                'language': 'en',\n",
    "                'country': 'USA'\n",
    "            },\n",
    "            'Reuters': {\n",
    "                'rss': 'https://www.reutersagency.com/feed/?best-topics=tech&post_type=best',\n",
    "                'web': 'https://www.reuters.com/',\n",
    "                'language': 'en',\n",
    "                'country': 'International'\n",
    "            },\n",
    "            'Al Jazeera': {\n",
    "                'rss': 'https://www.aljazeera.com/xml/rss/all.xml',\n",
    "                'web': 'https://www.aljazeera.com/news/',\n",
    "                'language': 'en',\n",
    "                'country': 'Qatar'\n",
    "            },\n",
    "            'Money Control': {\n",
    "                'rss': 'https://www.moneycontrol.com/rss/latestnews.xml',\n",
    "                'web': 'https://www.moneycontrol.com/news/',\n",
    "                'language': 'en',\n",
    "                'country': 'India'\n",
    "            },\n",
    "            'Aaj Tak': {\n",
    "                'rss': 'https://www.aajtak.in/feeds/default.rss',\n",
    "                'web': 'https://www.aajtak.in/',\n",
    "                'language': 'hi',\n",
    "                'country': 'India'\n",
    "            },\n",
    "            'Zee News': {\n",
    "                'rss': 'https://zeenews.india.com/rss/india-national-news.xml',\n",
    "                'web': 'https://zeenews.india.com/',\n",
    "                'language': 'hi',\n",
    "                'country': 'India'\n",
    "            },\n",
    "            'News18': {\n",
    "                'rss': 'https://www.news18.com/rss/india.xml',\n",
    "                'web': 'https://www.news18.com/',\n",
    "                'language': 'en',\n",
    "                'country': 'India'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.cache = set()\n",
    "\n",
    "    def _get_domain(self, url: str) -> str:\n",
    "        \"\"\"Extract domain from URL.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc.replace('www.', '')\n",
    "\n",
    "    def _make_request(self, url: str, max_retries: int = 3) -> Optional[requests.Response]:\n",
    "        \"\"\"Make HTTP request with retries.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                if response.status_code == 200:\n",
    "                    return response\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.error(f\"Request failed for {url}: {str(e)}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        return None\n",
    "\n",
    "    def _parse_date(self, date_str: str) -> datetime:\n",
    "        \"\"\"Parse various date formats.\"\"\"\n",
    "        if not date_str:\n",
    "            return datetime.min.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        formats = [\n",
    "            '%Y-%m-%dT%H:%M:%S%z',\n",
    "            '%a, %d %b %Y %H:%M:%S %z',\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%a, %d %b %Y %H:%M:%S GMT',\n",
    "            '%d %b %Y %H:%M:%S GMT',\n",
    "            '%B %d, %Y %I:%M %p %Z',\n",
    "            '%Y-%m-%d'\n",
    "        ]\n",
    "        \n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                dt = datetime.strptime(date_str, fmt)\n",
    "                if dt.tzinfo is None:\n",
    "                    return dt.replace(tzinfo=timezone.utc)\n",
    "                return dt.astimezone(timezone.utc)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        logger.warning(f\"Failed to parse date: {date_str}\")\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    def _analyze_text(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Perform NLP analysis on text to extract key topics.\"\"\"\n",
    "        if not text or not nlp:\n",
    "            return {}\n",
    "            \n",
    "        doc = nlp(text.lower())\n",
    "        \n",
    "        # Extract relevant nouns and proper nouns\n",
    "        nouns = [\n",
    "            token.lemma_ for token in doc \n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"] \n",
    "            and not token.is_stop\n",
    "            and len(token.text) > 2\n",
    "        ]\n",
    "        \n",
    "        # Count noun frequencies and calculate weights\n",
    "        noun_counts = Counter(nouns)\n",
    "        total_nouns = max(1, sum(noun_counts.values()))  # Avoid division by zero\n",
    "        \n",
    "        return {\n",
    "            noun: count/total_nouns \n",
    "            for noun, count in noun_counts.most_common(10)\n",
    "        }\n",
    "\n",
    "    def fetch_article_content(self, url: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract full article content including paragraphs and images.\"\"\"\n",
    "        domain = self._get_domain(url)\n",
    "        response = self._make_request(url)\n",
    "        \n",
    "        if not response:\n",
    "            return {\n",
    "                'summary': \"\",\n",
    "                'image': None,\n",
    "                'paragraphs': []\n",
    "            }\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Domain-specific content selectors\n",
    "        content_selectors = {\n",
    "            'hindustantimes.com': 'div.story-details',\n",
    "            'thehindu.com': 'div.articlebodycontent',\n",
    "            'indianexpress.com': 'div.main-content',\n",
    "            'bbc.com': 'div.article-body',\n",
    "            'cnn.com': 'div.article__content',\n",
    "            'reuters.com': 'div.article-body__content',\n",
    "            'aljazeera.com': 'div.wysiwyg--all-content',\n",
    "            'moneycontrol.com': 'div.article_section',\n",
    "            'aajtak.in': 'div.story-with-main-sec',\n",
    "            'zeenews.india.com': 'div.section-article',\n",
    "            'news18.com': 'article_story_content'\n",
    "        }\n",
    "        \n",
    "        # Extract paragraphs\n",
    "        paragraphs = []\n",
    "        content_selector = content_selectors.get(domain, 'body')\n",
    "        \n",
    "        try:\n",
    "            content = soup.select_one(content_selector) or soup\n",
    "            for p in content.find_all('p'):\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and len(text.split()) > 5:\n",
    "                    paragraphs.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting paragraphs: {e}\")\n",
    "        \n",
    "        # Extract main image\n",
    "        image = None\n",
    "        try:\n",
    "            # Try OpenGraph image first\n",
    "            og_image = soup.find('meta', property='og:image')\n",
    "            if og_image and og_image.get('content'):\n",
    "                image = og_image['content']\n",
    "            else:\n",
    "                # Fallback to first large image in content\n",
    "                for img in soup.find_all('img'):\n",
    "                    if int(img.get('width', 0)) > 300 or int(img.get('height', 0)) > 200:\n",
    "                        image = img.get('src')\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting image: {e}\")\n",
    "        \n",
    "        # Format the full content\n",
    "        full_content = '\\n\\n'.join(paragraphs) if paragraphs else \"\"\n",
    "        \n",
    "        return {\n",
    "            'summary': full_content[:2000] + ('...' if len(full_content) > 2000 else ''),\n",
    "            'image': image,\n",
    "            'paragraphs': paragraphs\n",
    "        }\n",
    "\n",
    "    def _generate_article_id(self, url: str, title: str) -> str:\n",
    "        \"\"\"Generate unique article ID.\"\"\"\n",
    "        return hashlib.md5(f\"{url}_{title}\".encode()).hexdigest()\n",
    "\n",
    "    def fetch_rss_news(self, source_name: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Fetch and process RSS feed entries.\"\"\"\n",
    "        if source_name not in self.news_sources:\n",
    "            return []\n",
    "            \n",
    "        rss_url = self.news_sources[source_name]['rss']\n",
    "        logger.info(f\"Fetching {source_name} RSS feed\")\n",
    "        \n",
    "        try:\n",
    "            feed = feedparser.parse(rss_url)\n",
    "            news_items = []\n",
    "            \n",
    "            for entry in feed.entries[:limit]:\n",
    "                article_id = self._generate_article_id(entry.link, entry.title)\n",
    "                if article_id in self.cache:\n",
    "                    continue\n",
    "                    \n",
    "                self.cache.add(article_id)\n",
    "                \n",
    "                # Skip entries with \"Today's news in 10 minutes\" in title\n",
    "                if \"Today's news in 10 minutes\" in entry.title:\n",
    "                    continue\n",
    "                \n",
    "                # Get full article content\n",
    "                article_content = self.fetch_article_content(entry.link)\n",
    "                \n",
    "                news_items.append({\n",
    "                    'id': article_id,\n",
    "                    'title': entry.title,\n",
    "                    'link': entry.link,\n",
    "                    'published': entry.get('published', ''),\n",
    "                    'source': source_name,\n",
    "                    'language': self.news_sources[source_name].get('language', 'en'),\n",
    "                    'country': self.news_sources[source_name].get('country', 'Unknown'),\n",
    "                    'summary': article_content['summary'],\n",
    "                    'paragraphs': article_content['paragraphs'],\n",
    "                    'image': article_content['image'],\n",
    "                })\n",
    "\n",
    "                \n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "                \n",
    "            return news_items\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {source_name} feed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_all_news(self, limit_per_source: int = 5) -> List[Dict]:\n",
    "        \"\"\"Aggregate news from all sources.\"\"\"\n",
    "        all_news = []\n",
    "        for source in self.news_sources:\n",
    "            try:\n",
    "                all_news.extend(self.fetch_rss_news(source, limit_per_source))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {source}: {e}\")\n",
    "                \n",
    "        # Sort by publication date (newest first)\n",
    "        all_news.sort(\n",
    "            key=lambda x: self._parse_date(x.get('published', '')),\n",
    "            reverse=True\n",
    "        )\n",
    "        return all_news\n",
    "\n",
    "class NewsProcessor:\n",
    "    \"\"\"Processes and structures news data with built-in text cleaning and merging.\"\"\"\n",
    "    \n",
    "    def __init__(self, scraper: NewsScraper):\n",
    "        self.scraper = scraper\n",
    "        self.df = pd.DataFrame(columns=[\n",
    "            'Article_ID',\n",
    "            'News',  # This will be our merged column\n",
    "            'Source',\n",
    "            'Language',\n",
    "            'Country',\n",
    "            'Published_Time',\n",
    "            'Access_Time',\n",
    "            'URL',\n",
    "            'Image_URL',\n",
    "        ])\n",
    "    \n",
    "    def _clean_and_merge_text(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean text columns and merge them into a single 'News' column.\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        # Define phrases to look for\n",
    "        ignore_phrases = [\n",
    "            \"Stay updated with the latest\",\n",
    "            \"Get the latest news\",\n",
    "            \"Read more at\",\n",
    "            \"For more updates\",\n",
    "            \"To learn more\",\n",
    "            \"Click here\",\n",
    "            \"Disclaimer:\",\n",
    "            \"For more lifestyle news\",\n",
    "            'A post shared by',\n",
    "            'Clickhere to follow',\n",
    "            '#WATCH',\n",
    "                    ]\n",
    "        \n",
    "        # Text columns to process\n",
    "        text_columns = ['Headline', 'Full_Content', 'Summary']\n",
    "        \n",
    "        # Compile regex pattern once\n",
    "        pattern = re.compile(rf'({\"|\".join(re.escape(p) for p in ignore_phrases)}).*', flags=re.IGNORECASE)\n",
    "        \n",
    "        # Clean all text columns\n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = (\n",
    "                    df[col]\n",
    "                    .replace([\"NaN\", pd.NA, None], \" \")\n",
    "                    .fillna(\" \")\n",
    "                    .astype(str)\n",
    "                    .apply(lambda x: re.sub(pattern, \"\", x).strip())\n",
    "                )\n",
    "        \n",
    "        # Merge columns into single 'News' column\n",
    "        df[\"News\"] = df[text_columns].agg(\" \".join, axis=1)\n",
    "        \n",
    "        # Remove content after 3 or more consecutive line breaks\n",
    "        df[\"News\"] = df[\"News\"].apply(lambda x: re.split(r'\\n{3,}', x)[0].strip())\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _generate_summary(self, paragraphs: List[str], max_paragraphs: int = 3) -> str:\n",
    "        \"\"\"Generate a summary from article paragraphs.\"\"\"\n",
    "        if not paragraphs:\n",
    "            return \"\"\n",
    "            \n",
    "        summary_paragraphs = paragraphs[:max_paragraphs]\n",
    "        summary = ' '.join(p.strip() for p in summary_paragraphs)\n",
    "        return summary[:500] + ('...' if len(summary) > 500 else '')\n",
    "\n",
    "    def _convert_to_ist(self, dt: datetime) -> datetime:\n",
    "        \"\"\"Convert datetime to Indian Standard Time (UTC+5:30).\"\"\"\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone(timedelta(hours=5, minutes=30)))\n",
    "\n",
    "    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean the dataframe by removing unwanted rows and empty content.\"\"\"\n",
    "        # Remove rows where Headline contains \"Today's news in 10 minutes\"\n",
    "        if 'Headline' in df.columns:\n",
    "            df = df[~df['Headline'].str.contains(\"Today's news in 10 minutes\", case=False, na=False)]\n",
    "        \n",
    "        # Remove rows with empty News content\n",
    "        if 'News' in df.columns:\n",
    "            df = df[df['News'].str.strip().astype(bool)]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def update_dataframe(self, news_items: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Update the DataFrame with new articles, clean text, and merge columns.\"\"\"\n",
    "        new_entries = []\n",
    "        current_time = self._convert_to_ist(datetime.now(timezone.utc))\n",
    "        \n",
    "        for item in news_items:\n",
    "            published_time = self._convert_to_ist(self.scraper._parse_date(item.get('published', '')))\n",
    "            \n",
    "            # Create entry with all original columns first\n",
    "            entry = {\n",
    "                'Article_ID': item['id'],\n",
    "                'Headline': item['title'],\n",
    "                'Full_Content': '\\n\\n'.join(item['paragraphs']),\n",
    "                'Summary': self._generate_summary(item['paragraphs']),\n",
    "                'Source': item['source'],\n",
    "                'Language': item['language'],\n",
    "                'Country': item['country'],\n",
    "                'Published_Time': published_time,\n",
    "                'Access_Time': current_time,\n",
    "                'URL': item['link'],\n",
    "                'Image_URL': item['image'],\n",
    "            }\n",
    "            new_entries.append(entry)\n",
    "        \n",
    "        if new_entries:\n",
    "            new_df = pd.DataFrame(new_entries)\n",
    "            # Clean and merge text columns\n",
    "            new_df = self._clean_and_merge_text(new_df)\n",
    "            # Remove temporary columns\n",
    "            new_df.drop(columns=['Headline', 'Full_Content', 'Summary'], inplace=True, errors='ignore')\n",
    "            # Apply additional cleaning\n",
    "            new_df = self._clean_dataframe(new_df)\n",
    "            # Merge with existing data\n",
    "            self.df = pd.concat([self.df, new_df]).drop_duplicates('Article_ID')\n",
    "            \n",
    "        return self.df\n",
    "\n",
    "    def get_latest_news_df(self, limit: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Get latest news and update DataFrame.\"\"\"\n",
    "        news_items = self.scraper.get_all_news(limit_per_source=max(1, limit//len(self.scraper.news_sources)))\n",
    "        return self.update_dataframe(news_items[:limit])\n",
    "\n",
    "class ContinuousNewsScraper:\n",
    "    def __init__(self, scrape_interval_minutes=2):\n",
    "        self.scraper = NewsScraper()\n",
    "        self.processor = NewsProcessor(self.scraper)\n",
    "        self.scrape_interval = scrape_interval_minutes * 60  # Convert to seconds\n",
    "        self.df_news = pd.DataFrame()  # Master DataFrame\n",
    "        self.seen_article_ids = set()  # Track duplicates\n",
    "\n",
    "    def get_new_articles(self, limit=20):\n",
    "        \"\"\"Fetch new articles, excluding duplicates.\"\"\"\n",
    "        new_news = self.processor.get_latest_news_df(limit=limit)\n",
    "        \n",
    "        if new_news.empty:\n",
    "            return pd.DataFrame()  # No new articles\n",
    "        \n",
    "        # Filter out already-seen articles\n",
    "        new_news = new_news[~new_news['Article_ID'].isin(self.seen_article_ids)]\n",
    "        \n",
    "        if not new_news.empty:\n",
    "            # Update seen articles\n",
    "            self.seen_article_ids.update(new_news['Article_ID'].tolist())\n",
    "            return new_news\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def run_continuous_scraping(self, max_iterations=None):\n",
    "        \"\"\"Run continuous scraping, appending only new news.\"\"\"\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            if max_iterations and iteration >= max_iterations:\n",
    "                break\n",
    "            \n",
    "            iteration += 1\n",
    "            print(f\"\\n=== Iteration {iteration} | {datetime.now().strftime('%H:%M:%S')} ===\")\n",
    "            \n",
    "            try:\n",
    "                new_articles = self.get_new_articles(limit=20)\n",
    "                \n",
    "                if not new_articles.empty:\n",
    "                    self.df_news = pd.concat([self.df_news, new_articles], ignore_index=True)\n",
    "                    print(f\"âœ… Added {len(new_articles)} new articles.\")\n",
    "                    print(\"Latest news:\")\n",
    "                    print(new_articles[['Headline', 'Source', 'Published_Time']].tail(3))\n",
    "                else:\n",
    "                    print(\"ðŸ”„ No new articles found.\")\n",
    "                \n",
    "                # Display stats\n",
    "                print(f\"\\nðŸ“Š Total articles: {len(self.df_news)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error: {e}\")\n",
    "            \n",
    "            time.sleep(self.scrape_interval)\n",
    "        \n",
    "        return self.df_news\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     scraper = ContinuousNewsScraper(scrape_interval_minutes=2)\n",
    "    \n",
    "#     try:\n",
    "#         # Run for 10 iterations (20 minutes)\n",
    "#         final_df = scraper.run_continuous_scraping(max_iterations=20)\n",
    "        \n",
    "#         # Save final results (optional)\n",
    "#         final_df.to_csv(\"latest_news_archive.csv\", index=False)\n",
    "#         print(\"\\nâœ… Scraping completed. Data saved to 'latest_news_archive.csv'\")\n",
    "    \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\nðŸ›‘ Manual stop detected. Saving current data...\")\n",
    "#         final_df = scraper.df_news\n",
    "#         final_df.to_csv(\"partial_news_archive.csv\", index=False)\n",
    "#         print(\"ðŸ’¾ Partial data saved to 'partial_news_archive.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f3fde-ef91-4b44-9b37-3d65388f22e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scraper = ContinuousNewsScraper(scrape_interval_minutes=2)  # Initialize scraper\n",
    "    \n",
    "    try:\n",
    "        # Run for **1 iteration only** (2 minutes)\n",
    "        final_df = scraper.run_continuous_scraping(max_iterations=0)  # Typo fixed: max_iterations\n",
    "        \n",
    "        # Save results\n",
    "        # final_df.to_csv(\"latest_news_archive.csv\", index=False)\n",
    "        # print(\"\\nâœ… Scraping completed (single run). Data saved to 'latest_news_archive.csv'\")\n",
    "        # print(final_df.head())  # Preview instead of full print\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        # print(\"\\nðŸ›‘ Manual stop detected. Saving partial data...\")\n",
    "        final_df = scraper.df_news\n",
    "        final_df.to_csv(\"partial_news_archive.csv\", index=False)\n",
    "        # print(\"ðŸ’¾ Partial data saved to 'partial_news_archive.csv'\")\n",
    "    \n",
    "    print(\"\\nâ­ï¸ Ready for the next step!\")  # Explicit breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151426dc-e960-4850-9810-9314d806929e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c487095-d880-47ab-b61a-eafcc781eba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3f916b-4292-405c-9f6a-dbae260b713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_Key = \"AIzaSyCO5DDWVA2gOUKsI3eJzu4tHo_yitnpWLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dde250-71f5-44d3-8118-c2a173118509",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Configuration\n",
    "API_KEY = API_Key  # Replace with your actual API key\n",
    "TEMPLATE_FOLDER = r\"E:/Intapost_Templates/Square Templetes 2/\"\n",
    "LOGO_PATH = r\"E:\\Intapost_Templates\\Square Templetes 2\\9.png\"\n",
    "\n",
    "# Gemini AI Initialization\n",
    "def initialize_gemini(api_key):\n",
    "    \"\"\"Initialize Gemini AI with API key and model selection.\"\"\"\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    model_names_to_try = [\n",
    "        'gemini-1.5-flash',\n",
    "        'gemini-1.5-pro',\n",
    "        'gemini-pro'\n",
    "    ]\n",
    "    \n",
    "    for model_name in model_names_to_try:\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(\"Hello\")\n",
    "            if response.text:\n",
    "                print(f\"âœ… Using model: {model_name}\")\n",
    "                return model\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Model {model_name} failed: {str(e)}\")\n",
    "    \n",
    "    raise ValueError(\"âŒ No working model found. Check API key/model availability.\")\n",
    "\n",
    "# Initialize model globally\n",
    "try:\n",
    "    model = initialize_gemini(API_KEY)\n",
    "except Exception as e:\n",
    "    print(f\"Initialization failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "# News Processing\n",
    "ENFORCED_PROMPT = \"\"\"\n",
    "Transform news snippets into concise 60-word articles with:\n",
    "\n",
    "1. STRUCTURE:\n",
    "Headline: [5-12 words with key terms]\n",
    "Category: [Predefined category]\n",
    "State: [Indian state or 'National' or 'International']\n",
    "Subheading: [Core news in one sentence]\n",
    "Content: [All key details within 60 words total]\n",
    "\n",
    "2. CATEGORIES:\n",
    "- Crime/Legal\n",
    "- Politics/Government\n",
    "- Business/Economy\n",
    "- Health/Medicine\n",
    "- Education/Research\n",
    "- Technology/Science\n",
    "- Environment/Climate\n",
    "- International\n",
    "- Human Interest\n",
    "- Sports/Entertainment\n",
    "\n",
    "3. RULES:\n",
    "- Bold key terms: **Rs 500 crore scam**\n",
    "- Preserve ALL critical facts (names, figures, locations)\n",
    "- Neutral tone for news, analytical for opinions\n",
    "- For investigations: Highlight methods â†’ findings â†’ consequences\n",
    "- For opinions: Start with \"PERSPECTIVE:\"\n",
    "\"\"\"\n",
    "\n",
    "def process_news_dataframe(df):\n",
    "    \"\"\"End-to-end news processing pipeline.\"\"\"\n",
    "    \n",
    "    def identify_state(text):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                f\"Analyze text and return ONLY:\\nState: [Indian state or 'National' or 'International']\\n\\nText: {text}\"\n",
    "            )\n",
    "            return response.text.strip() if response.text else \"National\"\n",
    "        except:\n",
    "            return \"National\"\n",
    "\n",
    "    def identify_category(text):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                f\"Classify into: Crime/Legal, Politics/Government, Business/Economy, Health/Medicine, Education/Research, Technology/Science, Environment/Climate, International, Human Interest, Sports/Entertainment. Reply ONLY with category.\\n\\nText: {text}\"\n",
    "            )\n",
    "            return response.text.strip() if response.text else \"General\"\n",
    "        except:\n",
    "            return \"General\"\n",
    "\n",
    "    def enforce_word_limit(text, limit=60):\n",
    "        return ' '.join(text.split()[:limit])\n",
    "\n",
    "    def transform_snippet(snippet):\n",
    "        try:\n",
    "            category = identify_category(snippet)\n",
    "            state = identify_state(snippet)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "\n",
    "            response = model.generate_content(\n",
    "                f\"{ENFORCED_PROMPT}\\n\\nInput News:\\n{snippet}\\n\\nCategory: {category}\\nState: {state}\"\n",
    "            )\n",
    "            time.sleep(1)\n",
    "\n",
    "            if not response.text:\n",
    "                return \"Headline: \\nCategory: \\nState: \\nSubheading: \\nContent: \"\n",
    "\n",
    "            structured = response.text\n",
    "            if \"Content:\" in structured:\n",
    "                parts = structured.split(\"Content:\")\n",
    "                content = enforce_word_limit(parts[1].strip())\n",
    "                return f\"{parts[0].strip()}\\nContent: {content}\"\n",
    "            return structured\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Processing error: {e}\")\n",
    "            return \"Headline: \\nCategory: \\nState: \\nSubheading: \\nContent: \"\n",
    "\n",
    "    results = []\n",
    "    for snippet in df[\"News\"]:\n",
    "        transformed = transform_snippet(snippet)\n",
    "        results.append(transformed)\n",
    "        # print(\"Processed:\", transformed[:100] + \"...\")\n",
    "\n",
    "    def extract_fields(text):\n",
    "        text = re.sub(r'\\*+', '', text)\n",
    "        fields = {\n",
    "            \"Headline\": re.search(r'Headline:\\s*(.*?)\\n', text).group(1).strip() if re.search(r'Headline:', text) else \"\",\n",
    "            \"Category\": re.search(r'Category:\\s*(.*?)\\n', text).group(1).strip() if re.search(r'Category:', text) else \"\",\n",
    "            \"State\": re.search(r'State:\\s*(.*?)\\n', text).group(1).strip() if re.search(r'State:', text) else \"\",\n",
    "            \"Subheading\": re.search(r'Subheading:\\s*(.*?)\\n', text).group(1).strip() if re.search(r'Subheading:', text) else \"\",\n",
    "            \"Content\": re.search(r'Content:\\s*(.*)', text).group(1).strip() if re.search(r'Content:', text) else \"\"\n",
    "        }\n",
    "        return fields\n",
    "\n",
    "    extracted_data = [extract_fields(item) for item in results]\n",
    "    final_df = pd.concat([df.reset_index(drop=True), pd.DataFrame(extracted_data)], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Image Processing Functions\n",
    "def mm_to_pixels(mm, dpi=600):\n",
    "    return int(mm * dpi / 25.4)\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGBA\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from URL: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_edge_transparency(img, fade_percent=20):\n",
    "    width, height = img.size\n",
    "    fade_width = int(width * fade_percent / 100)\n",
    "    fade_height = int(height * fade_percent / 100)\n",
    "    \n",
    "    alpha = Image.new('L', (width, height), 255)\n",
    "    draw = ImageDraw.Draw(alpha)\n",
    "    \n",
    "    for x in range(fade_width):\n",
    "        opacity = int(255 * (x / fade_width))\n",
    "        draw.line([(x, 0), (x, height)], fill=opacity)\n",
    "    \n",
    "    for x in range(width - fade_width, width):\n",
    "        opacity = int(255 * ((width - x) / fade_width))\n",
    "        draw.line([(x, 0), (x, height)], fill=opacity)\n",
    "    \n",
    "    for y in range(fade_height):\n",
    "        opacity = int(255 * (y / fade_height))\n",
    "        draw.line([(0, y), (width, y)], fill=opacity)\n",
    "    \n",
    "    for y in range(height - fade_height, height):\n",
    "        opacity = int(255 * ((height - y) / fade_height))\n",
    "        draw.line([(0, y), (width, y)], fill=opacity)\n",
    "    \n",
    "    img.putalpha(alpha)\n",
    "    return img\n",
    "\n",
    "def add_bottom_gradient(img, fade_height_percent=50):\n",
    "    width, height = img.size\n",
    "    fade_height = int(height * fade_height_percent / 100)\n",
    "    \n",
    "    gradient = Image.new('L', (width, fade_height))\n",
    "    for y in range(fade_height):\n",
    "        alpha = int(255 * (1.2 * y / fade_height)**2)\n",
    "        gradient.paste(alpha, (0, y, width, y+1))\n",
    "    \n",
    "    black_layer = Image.new('RGB', (width, fade_height), (0, 0, 0))\n",
    "    black_layer.putalpha(gradient)\n",
    "    \n",
    "    result = img.convert('RGBA')\n",
    "    result.paste(black_layer, (0, height - fade_height), black_layer)\n",
    "    return result\n",
    "\n",
    "def update_instagram_post(image_path, output_path, headline, subheading, subsubheading, \n",
    "                         logo_path=None, image_url=None, source=\"\", publish_date=\"\", category=\"\", state=\"\"):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = img.size\n",
    "    \n",
    "    if image_url:\n",
    "        url_image = load_image_from_url(image_url)\n",
    "        if url_image:\n",
    "            side_margin = mm_to_pixels(10)\n",
    "            max_image_width = width - 2 * side_margin\n",
    "            \n",
    "            url_img_width, url_img_height = url_image.size\n",
    "            scale_factor = min(2.5, max_image_width / url_img_width)\n",
    "            new_width = int(url_img_width * scale_factor)\n",
    "            new_height = int(url_img_height * scale_factor)\n",
    "            \n",
    "            url_image = url_image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "            url_image = add_edge_transparency(url_image)\n",
    "            \n",
    "            x_position = side_margin + (max_image_width - new_width) // 2\n",
    "            y_position = (height - new_height) // 2\n",
    "            \n",
    "            url_layer = Image.new('RGBA', img.size, (0, 0, 0, 0))\n",
    "            url_layer.paste(url_image, (x_position, y_position))\n",
    "            \n",
    "            img = Image.alpha_composite(img.convert('RGBA'), url_layer)\n",
    "    \n",
    "    img = add_bottom_gradient(img, 40)\n",
    "    \n",
    "    logo_margin = 5\n",
    "    if logo_path:\n",
    "        try:\n",
    "            logo = Image.open(logo_path).convert(\"RGBA\")\n",
    "            logo_size = mm_to_pixels(15)\n",
    "            logo_height = int(logo_size * (logo.size[1] / logo.size[0]))\n",
    "            logo = logo.resize((logo_size, logo_height), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            logo_layer = Image.new('RGBA', img.size, (0, 0, 0, 0))\n",
    "            logo_layer.paste(logo, (mm_to_pixels(logo_margin), mm_to_pixels(logo_margin)), logo)\n",
    "            \n",
    "            img = Image.alpha_composite(img, logo_layer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading logo: {e}\")\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    State_Size = 60\n",
    "    Category_Size = 120\n",
    "    Head_Size = 80\n",
    "    SbHead_Size = 75\n",
    "    B_Size = 50\n",
    "    Source_Size = 40\n",
    "    \n",
    "    state_color = \"red\"\n",
    "    category_color = \"white\"\n",
    "    headline_color = \"white\"\n",
    "    subheading_color = 'red'\n",
    "    subsubheading_color = \"white\" \n",
    "    source_color = (200, 200, 200)\n",
    "    box_fill = (112, 128, 144, 0)\n",
    "    box_outline = \"#000000\"\n",
    "    box_outline_width = mm_to_pixels(0.08)\n",
    "    text_stroke_width = 2\n",
    "\n",
    "    category_headline_space = 30\n",
    "    headline_subheading_space = 60\n",
    "    subheading_subsubheading_space = 40\n",
    "    line_spacing = 2.5\n",
    "    margin_px = 100\n",
    "    max_text_width = width - (1 + 2 * margin_px)\n",
    "    radius_px = mm_to_pixels(1.5)\n",
    "    box_margin = 25\n",
    "    source_margin = 20\n",
    "\n",
    "    def load_font(font_paths, size):\n",
    "        for path in font_paths:\n",
    "            try:\n",
    "                return ImageFont.truetype(path, size)\n",
    "            except:\n",
    "                continue\n",
    "        return ImageFont.load_default(size)\n",
    "\n",
    "    font_paths = [\n",
    "        \"times.ttf\",\n",
    "        \"arial.ttf\",\n",
    "        \"arialbd.ttf\"\n",
    "    ]\n",
    "    \n",
    "    state_font = load_font(font_paths, State_Size)\n",
    "    category_font = load_font(font_paths, Category_Size)\n",
    "    headline_font = load_font(font_paths, Head_Size)\n",
    "    subhead_font = load_font(font_paths, SbHead_Size)\n",
    "    body_font = load_font(font_paths, B_Size)\n",
    "    source_font = load_font(font_paths, Source_Size)\n",
    "\n",
    "    if state:\n",
    "        state_text = f\"{state.capitalize()}\"  # \"Gujarat\" instead of \"GUJARAT\"\n",
    "        state_x = mm_to_pixels(logo_margin) + 170  # Aligns with logo's left edge (default: 5mm)\n",
    "        state_y = mm_to_pixels(logo_margin) + mm_to_pixels(20) - 125  # Below logo\n",
    "        \n",
    "        draw.text(\n",
    "            (state_x, state_y),\n",
    "            state_text,\n",
    "            font=state_font,\n",
    "            fill=state_color,\n",
    "            anchor=\"mm\" # \"la\" # Left-aligned\n",
    "        )\n",
    "\n",
    "    def get_text_height(text, font):\n",
    "        if not text or pd.isna(text):\n",
    "            return 0\n",
    "        lines = textwrap.wrap(str(text), width=int(max_text_width/(font.size*0.4)))\n",
    "        return (font.size + line_spacing) * len(lines)\n",
    "\n",
    "    category_lines = textwrap.wrap(category.upper(), width=int(max_text_width/(Category_Size*0.7)))\n",
    "    category_height = (Category_Size + line_spacing) * len(category_lines)\n",
    "    headline_lines = textwrap.wrap(headline.upper(), width=int(max_text_width/(Head_Size*0.7)))\n",
    "    headline_height = (Head_Size + line_spacing) * len(headline_lines)\n",
    "    subhead_height = get_text_height(subheading, subhead_font)\n",
    "    subsub_lines = textwrap.wrap(subsubheading, width=int(max_text_width/(B_Size*0.5)))\n",
    "    subsub_height = (B_Size + line_spacing) * len(subsub_lines)\n",
    "\n",
    "    min_content_space = (category_height + category_headline_space +\n",
    "                        headline_height + headline_subheading_space + \n",
    "                        subhead_height + subheading_subsubheading_space + \n",
    "                        subsub_height + 2*box_margin)\n",
    "    \n",
    "    gradient_top = height - int(height * 0.4)\n",
    "    available_space = gradient_top - margin_px\n",
    "    \n",
    "    if min_content_space > available_space:\n",
    "        scale_factor = available_space / min_content_space\n",
    "        Category_Size = int(Category_Size * scale_factor * 0.9)\n",
    "        Head_Size = int(Head_Size * scale_factor * 0.9)\n",
    "        SbHead_Size = int(SbHead_Size * scale_factor * 0.9)\n",
    "        B_Size = int(B_Size * scale_factor * 0.9)\n",
    "        \n",
    "        category_font = load_font(font_paths, Category_Size)\n",
    "        headline_font = load_font(font_paths, Head_Size)\n",
    "        subhead_font = load_font(font_paths, SbHead_Size)\n",
    "        body_font = load_font(font_paths, B_Size)\n",
    "        \n",
    "        category_lines = textwrap.wrap(category.upper(), width=int(max_text_width/(Category_Size*0.7)))\n",
    "        category_height = (Category_Size + line_spacing) * len(category_lines)\n",
    "        headline_lines = textwrap.wrap(headline.upper(), width=int(max_text_width/(Head_Size*0.7)))\n",
    "        headline_height = (Head_Size + line_spacing) * len(headline_lines)\n",
    "        subhead_height = get_text_height(str(subheading), subhead_font) if subheading else 0\n",
    "        subsub_lines = textwrap.wrap(str(subsubheading), width=int(max_text_width/(B_Size*0.5))) if subsubheading else []\n",
    "        subsub_height = (B_Size + line_spacing) * len(subsub_lines)\n",
    "\n",
    "    current_y = height - margin_px - (category_height + category_headline_space +\n",
    "                                    headline_height + headline_subheading_space + \n",
    "                                    subhead_height + subheading_subsubheading_space + \n",
    "                                    subsub_height + 2*box_margin)\n",
    "    \n",
    "    min_y = mm_to_pixels(logo_margin) + (mm_to_pixels(25) if logo_path else 0) + 20\n",
    "    current_y = max(current_y, min_y)\n",
    "\n",
    "    box_x1 = margin_px - box_margin\n",
    "    box_x2 = width - margin_px + box_margin\n",
    "    box_y1 = current_y + category_height + category_headline_space + headline_height + headline_subheading_space + subhead_height + subheading_subsubheading_space - box_margin\n",
    "    box_y2 = box_y1 + subsub_height + 2 * box_margin\n",
    "    box_y2 = min(box_y2, height - 10)\n",
    "\n",
    "    box_layer = Image.new('RGBA', img.size, (0, 0, 0, 0))\n",
    "    box_draw = ImageDraw.Draw(box_layer)\n",
    "    box_draw.rounded_rectangle(\n",
    "        [box_x1, box_y1, box_x2, box_y2],\n",
    "        radius=radius_px,\n",
    "        fill=box_fill,\n",
    "        outline=box_outline,\n",
    "        width=box_outline_width\n",
    "    )\n",
    "    img = Image.alpha_composite(img, box_layer)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    def draw_text_with_stroke(x, y, text, font, fill, stroke_fill=\"black\", stroke_width=2):\n",
    "        anchor = 'lm'\n",
    "        for dx in [-stroke_width, stroke_width]:\n",
    "            for dy in [-stroke_width, stroke_width]:\n",
    "                draw.text((x + dx, y + dy), text, font=font, fill=stroke_fill, anchor=anchor)\n",
    "        draw.text((x, y), text, font=font, fill=fill, anchor=anchor)\n",
    "\n",
    "    for line in category_lines:\n",
    "        text_width = category_font.getlength(line)\n",
    "        text_height = Category_Size\n",
    "        \n",
    "        bg_y1 = current_y - (text_height // 3) - 11\n",
    "        bg_y2 = current_y + (text_height * 2) // 3 - 11\n",
    "        bg_x1 = margin_px - 10\n",
    "        bg_x2 = margin_px + text_width + 10\n",
    "        \n",
    "        draw.rectangle(\n",
    "            [bg_x1, bg_y1, bg_x2, bg_y2],\n",
    "            fill=\"lime\",\n",
    "            outline=None\n",
    "        )\n",
    "        \n",
    "        draw_text_with_stroke(\n",
    "            margin_px, current_y, line,\n",
    "            font=category_font,\n",
    "            fill=category_color,\n",
    "            stroke_width=text_stroke_width\n",
    "        )\n",
    "        current_y += Category_Size + line_spacing\n",
    "    \n",
    "    current_y += category_headline_space\n",
    "\n",
    "    for line in headline_lines:\n",
    "        draw_text_with_stroke(\n",
    "            margin_px, current_y, line,\n",
    "            font=headline_font,\n",
    "            fill=headline_color,\n",
    "            stroke_width=text_stroke_width\n",
    "        )\n",
    "        current_y += Head_Size + line_spacing\n",
    "    \n",
    "    current_y += headline_subheading_space - line_spacing\n",
    "    \n",
    "    for line in textwrap.wrap(subheading, width=int(max_text_width/(SbHead_Size*0.5))):\n",
    "        draw_text_with_stroke(\n",
    "            margin_px, current_y, line,\n",
    "            font=subhead_font,\n",
    "            fill=subheading_color,\n",
    "            stroke_width=text_stroke_width\n",
    "        )\n",
    "        current_y += SbHead_Size + line_spacing\n",
    "    \n",
    "    current_y += subheading_subsubheading_space\n",
    "    \n",
    "    def draw_justified(text, font, start_y, color, max_height, original_font_size):\n",
    "        y = start_y\n",
    "        current_font_size = original_font_size\n",
    "        font = load_font(font_paths, current_font_size)\n",
    "        line_height = current_font_size + line_spacing\n",
    "        lines = []\n",
    "        min_font_size = int(original_font_size * 0.6)\n",
    "    \n",
    "        wrapped_lines = textwrap.wrap(text, width=int(max_text_width / (current_font_size * 0.5)))\n",
    "        required_height = len(wrapped_lines) * line_height\n",
    "    \n",
    "        if required_height <= max_height:\n",
    "            lines = wrapped_lines\n",
    "        else:\n",
    "            while current_font_size >= min_font_size and required_height > max_height:\n",
    "                current_font_size -= 1\n",
    "                font = load_font(font_paths, current_font_size)\n",
    "                line_height = current_font_size + line_spacing\n",
    "                wrapped_lines = textwrap.wrap(text, width=int(max_text_width / (current_font_size * 0.5)))\n",
    "                required_height = len(wrapped_lines) * line_height\n",
    "    \n",
    "            lines = wrapped_lines\n",
    "    \n",
    "        if required_height > max_height and lines:\n",
    "            lines = wrapped_lines[:max_height // line_height]\n",
    "            if lines:\n",
    "                lines[-1] = lines[-1][:max(0, len(lines[-1])-3)] + \"...\"\n",
    "    \n",
    "        y = start_y\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                total_width = sum(font.getlength(word) for word in words)\n",
    "                space_width = (max_text_width - total_width) / (len(words) - 1)\n",
    "                x = margin_px\n",
    "                for word in words[:-1]:\n",
    "                    draw.text((x, y), word, font=font, fill=color, anchor='lm')\n",
    "                    x += font.getlength(word) + space_width\n",
    "                draw.text((x, y), words[-1], font=font, fill=color, anchor='lm')\n",
    "            else:\n",
    "                draw.text((margin_px, y), line, font=font, fill=color, anchor='lm')\n",
    "            y += line_height\n",
    "    \n",
    "    draw_justified(subsubheading, body_font, current_y, subsubheading_color, \n",
    "                  box_y2 - current_y, B_Size)\n",
    "\n",
    "    if source or publish_date:\n",
    "        source_text = f\"Source: {source}\" if source else \"\"\n",
    "        date_text = f\"Published: {publish_date}\" if publish_date else \"\"\n",
    "        info_text = \" | \".join(filter(None, [source_text, date_text]))\n",
    "        \n",
    "        text_width = source_font.getlength(info_text)\n",
    "        text_x = width - margin_px - text_width\n",
    "        text_y = height - margin_px // 2\n",
    "        \n",
    "        draw.text(\n",
    "            (text_x, text_y),\n",
    "            info_text,\n",
    "            font=source_font,\n",
    "            fill=source_color,\n",
    "            anchor=\"lm\"\n",
    "        )\n",
    "\n",
    "    if output_path:\n",
    "        img.convert(\"RGB\").save(output_path, quality=95)\n",
    "    return img\n",
    "\n",
    "# Main Execution\n",
    "def process_and_generate_images(df):\n",
    "    # Process news data\n",
    "    processed_df = process_news_dataframe(df)\n",
    "    \n",
    "    # Generate images for each news item\n",
    "    for i in range(processed_df.shape[0]):\n",
    "        headline = str(processed_df['Headline'].iloc[i]) if pd.notna(processed_df['Headline'].iloc[i]) else \"\"\n",
    "        subheading = str(processed_df['Subheading'].iloc[i]) if pd.notna(processed_df['Subheading'].iloc[i]) else \"\"\n",
    "        subsubheading = str(processed_df['Content'].iloc[i]) if pd.notna(processed_df['Content'].iloc[i]) else \"\"\n",
    "        source = str(processed_df['Source'].iloc[i]) if pd.notna(processed_df['Source'].iloc[i]) else \"\"\n",
    "        publish_date = str(processed_df['Published_Time'].iloc[i]) if pd.notna(processed_df['Published_Time'].iloc[i]) else \"\"\n",
    "        category = str(processed_df['Category'].iloc[i]) if pd.notna(processed_df['Category'].iloc[i]) else \"\"\n",
    "        state = str(processed_df['State'].iloc[i]) if pd.notna(processed_df['State'].iloc[i]) else \"\"\n",
    "        \n",
    "        # Check if URL is valid\n",
    "        image_url = processed_df['Image_URL'].iloc[i]\n",
    "        is_valid_url = False\n",
    "        if pd.notna(image_url):\n",
    "            try:\n",
    "                parsed = urlparse(str(image_url))\n",
    "                is_valid_url = all([parsed.scheme in ['http','https'], parsed.netloc])\n",
    "            except:\n",
    "                is_valid_url = False\n",
    "        \n",
    "        # Generate random template path\n",
    "        template_path = f\"{TEMPLATE_FOLDER}{random.randint(1, 6)}.png\"\n",
    "        \n",
    "        # Create Instagram post\n",
    "        update_instagram_post(        \n",
    "            image_path=template_path,\n",
    "            output_path=f\"output_image_{i}.jpg\",\n",
    "            category=category,\n",
    "            headline=headline,\n",
    "            subheading=subheading,\n",
    "            subsubheading=subsubheading,\n",
    "            logo_path=LOGO_PATH,\n",
    "            image_url=image_url if is_valid_url else None,\n",
    "            source=source,\n",
    "            publish_date=publish_date,\n",
    "            state=state\n",
    "        )\n",
    "        print(f\"Generated image for news item {i+1}\")\n",
    "\n",
    "# Example usage (assuming df is your input DataFrame)\n",
    "# process_and_generate_images(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2387f-a85b-4056-852c-900547ba2838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_and_generate_images(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67751b6c-f7b1-4f4f-8ee3-d377a720585d",
   "metadata": {},
   "source": [
    "## New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d53283-b756-4e03-b288-d9cc82d7b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pytz import timezone\n",
    "import hashlib\n",
    "from typing import Dict, List, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5069f31-8499-4c99-bd1a-604d6ac98804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using model: gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "API_KEY =  API_Key#\"YOUR_API_KEY\"  # Replace with your actual API key\n",
    "TEMPLATE_FOLDER = \"E:/Intapost_Templates/Square Templetes 2/\"\n",
    "LOGO_PATH = r\"E:\\Intapost_Templates\\Square Templetes 2\\9.png\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('news_scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load English language model for NLP\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    logger.error(\"Spacy model 'en_core_web_sm' not found. Please install it first.\")\n",
    "    logger.info(\"Run: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Gemini AI Initialization\n",
    "def initialize_gemini(api_key):\n",
    "    \"\"\"Initialize Gemini AI with API key and model selection.\"\"\"\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    model_names_to_try = [\n",
    "        'gemini-1.5-flash',\n",
    "        'gemini-1.5-pro',\n",
    "        'gemini-pro'\n",
    "    ]\n",
    "    \n",
    "    for model_name in model_names_to_try:\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(\"Hello\")\n",
    "            if response.text:\n",
    "                print(f\"âœ… Using model: {model_name}\")\n",
    "                return model\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Model {model_name} failed: {str(e)}\")\n",
    "    \n",
    "    raise ValueError(\"âŒ No working model found. Check API key/model availability.\")\n",
    "\n",
    "# Initialize model globally\n",
    "try:\n",
    "    model = initialize_gemini(API_KEY)\n",
    "except Exception as e:\n",
    "    print(f\"Initialization failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "# News Processing\n",
    "ENFORCED_PROMPT = \"\"\"\n",
    "Transform news snippets into concise 60-word articles with:\n",
    "\n",
    "1. STRUCTURE:\n",
    "Headline: [5-12 words with key terms]\n",
    "Category: [Predefined category]\n",
    "State: [Indian state or 'National' or 'International']\n",
    "Subheading: [Core news in one sentence]\n",
    "Content: [All key details within 60 words total]\n",
    "\n",
    "2. CATEGORIES:\n",
    "- Crime/Legal\n",
    "- Politics/Government\n",
    "- Business/Economy\n",
    "- Health/Medicine\n",
    "- Education/Research\n",
    "- Technology/Science\n",
    "- Environment/Climate\n",
    "- International\n",
    "- Human Interest\n",
    "- Sports/Entertainment\n",
    "\n",
    "3. RULES:\n",
    "- Bold key terms: **Rs 500 crore scam**\n",
    "- Preserve ALL critical facts (names, figures, locations)\n",
    "- Neutral tone for news, analytical for opinions\n",
    "- For investigations: Highlight methods â†’ findings â†’ consequences\n",
    "- For opinions: Start with \"PERSPECTIVE:\"\n",
    "\"\"\"\n",
    "\n",
    "class NewsScraper:\n",
    "    \"\"\"A robust news scraper that collects articles from multiple RSS feeds.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'TE': 'Trailers'\n",
    "        }\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.session.max_redirects = 5\n",
    "        self.timeout = 15\n",
    "        \n",
    "        self.news_sources = {\n",
    "            'Hindustan Times': {'rss': 'https://www.hindustantimes.com/feeds/rss/latest-news/rssfeed.xml'},\n",
    "            'The Hindu': {'rss': 'https://www.thehindu.com/feeder/default.rss'},\n",
    "            'Indian Express': {'rss': 'https://indianexpress.com/feed/'},\n",
    "            'BBC': {'rss': 'http://feeds.bbci.co.uk/news/rss.xml'},\n",
    "            'CNN': {'rss': 'http://rss.cnn.com/rss/cnn_latest.rss'},\n",
    "            'Reuters': {'rss': 'https://www.reutersagency.com/feed/?best-topics=tech&post_type=best'},\n",
    "            'Al Jazeera': {'rss': 'https://www.aljazeera.com/xml/rss/all.xml'},\n",
    "            'Money Control': {'rss': 'https://www.moneycontrol.com/rss/latestnews.xml'},\n",
    "            'News18': {'rss': 'https://www.news18.com/rss/india.xml'}\n",
    "        }\n",
    "        \n",
    "        self.cache = set()\n",
    "\n",
    "    def _get_domain(self, url: str) -> str:\n",
    "        \"\"\"Extract domain from URL.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc.replace('www.', '')\n",
    "\n",
    "    def _make_request(self, url: str, max_retries: int = 3) -> Optional[requests.Response]:\n",
    "        \"\"\"Make HTTP request with retries.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                if response.status_code == 200:\n",
    "                    return response\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.error(f\"Request failed for {url}: {str(e)}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        return None\n",
    "\n",
    "    def fetch_article_content(self, url: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract full article content including paragraphs and images.\"\"\"\n",
    "        domain = self._get_domain(url)\n",
    "        response = self._make_request(url)\n",
    "        \n",
    "        if not response:\n",
    "            return {'summary': \"\", 'image': None, 'paragraphs': []}\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract paragraphs\n",
    "        paragraphs = []\n",
    "        try:\n",
    "            for p in soup.find_all('p'):\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and len(text.split()) > 5:\n",
    "                    paragraphs.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting paragraphs: {e}\")\n",
    "        \n",
    "        # Extract main image\n",
    "        image = None\n",
    "        try:\n",
    "            og_image = soup.find('meta', property='og:image')\n",
    "            if og_image and og_image.get('content'):\n",
    "                image = og_image['content']\n",
    "            else:\n",
    "                for img in soup.find_all('img'):\n",
    "                    if int(img.get('width', 0)) > 300 or int(img.get('height', 0)) > 200:\n",
    "                        image = img.get('src')\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting image: {e}\")\n",
    "        \n",
    "        full_content = '\\n\\n'.join(paragraphs) if paragraphs else \"\"\n",
    "        \n",
    "        return {\n",
    "            'summary': full_content[:2000] + ('...' if len(full_content) > 2000 else ''),\n",
    "            'image': image,\n",
    "            'paragraphs': paragraphs\n",
    "        }\n",
    "\n",
    "    def _generate_article_id(self, url: str, title: str) -> str:\n",
    "        \"\"\"Generate unique article ID.\"\"\"\n",
    "        return hashlib.md5(f\"{url}_{title}\".encode()).hexdigest()\n",
    "\n",
    "    def fetch_rss_news(self, source_name: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Fetch and process RSS feed entries.\"\"\"\n",
    "        if source_name not in self.news_sources:\n",
    "            return []\n",
    "            \n",
    "        rss_url = self.news_sources[source_name]['rss']\n",
    "        logger.info(f\"Fetching {source_name} RSS feed\")\n",
    "        \n",
    "        try:\n",
    "            feed = feedparser.parse(rss_url)\n",
    "            news_items = []\n",
    "            \n",
    "            for entry in feed.entries[:limit]:\n",
    "                article_id = self._generate_article_id(entry.link, entry.title)\n",
    "                if article_id in self.cache:\n",
    "                    continue\n",
    "                    \n",
    "                self.cache.add(article_id)\n",
    "                \n",
    "                if \"Today's news in 10 minutes\" in entry.title:\n",
    "                    continue\n",
    "                \n",
    "                article_content = self.fetch_article_content(entry.link)\n",
    "                \n",
    "                news_items.append({\n",
    "                    'id': article_id,\n",
    "                    'title': entry.title,\n",
    "                    'link': entry.link,\n",
    "                    'published': entry.get('published', ''),\n",
    "                    'source': source_name,\n",
    "                    'summary': article_content['summary'],\n",
    "                    'paragraphs': article_content['paragraphs'],\n",
    "                    'image': article_content['image'],\n",
    "                })\n",
    "                \n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            return news_items\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {source_name} feed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_all_news(self, limit_per_source: int = 5) -> List[Dict]:\n",
    "        \"\"\"Aggregate news from all sources.\"\"\"\n",
    "        all_news = []\n",
    "        for source in self.news_sources:\n",
    "            try:\n",
    "                all_news.extend(self.fetch_rss_news(source, limit_per_source))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {source}: {e}\")\n",
    "                \n",
    "        return all_news\n",
    "\n",
    "def process_news_data(news_items: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Process raw news items into structured DataFrame.\"\"\"\n",
    "    processed_news = []\n",
    "    \n",
    "    def identify_state(text):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                f\"Analyze text and return ONLY:\\nState: [Indian state or 'National' or 'International']\\n\\nText: {text}\"\n",
    "            )\n",
    "            return response.text.strip() if response.text else \"National\"\n",
    "        except:\n",
    "            return \"National\"\n",
    "\n",
    "    def identify_category(text):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                f\"Classify into: Crime/Legal, Politics/Government, Business/Economy, Health/Medicine, Education/Research, Technology/Science, Environment/Climate, International, Human Interest, Sports/Entertainment. Reply ONLY with category.\\n\\nText: {text}\"\n",
    "            )\n",
    "            return response.text.strip() if response.text else \"General\"\n",
    "        except:\n",
    "            return \"General\"\n",
    "\n",
    "    def enforce_word_limit(text, limit=60):\n",
    "        return ' '.join(text.split()[:limit])\n",
    "\n",
    "    def transform_snippet(snippet):\n",
    "        try:\n",
    "            category = identify_category(snippet)\n",
    "            state = identify_state(snippet)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "\n",
    "            response = model.generate_content(\n",
    "                f\"{ENFORCED_PROMPT}\\n\\nInput News:\\n{snippet}\\n\\nCategory: {category}\\nState: {state}\"\n",
    "            )\n",
    "            time.sleep(1)\n",
    "\n",
    "            if not response.text:\n",
    "                return \"Headline: \\nCategory: \\nState: \\nSubheading: \\nContent: \"\n",
    "\n",
    "            structured = response.text\n",
    "            if \"Content:\" in structured:\n",
    "                parts = structured.split(\"Content:\")\n",
    "                content = enforce_word_limit(parts[1].strip())\n",
    "                return f\"{parts[0].strip()}\\nContent: {content}\"\n",
    "            return structured\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Processing error: {e}\")\n",
    "            return \"Headline: \\nCategory: \\nState: \\nSubheading: \\nContent: \"\n",
    "\n",
    "    for item in news_items:\n",
    "        snippet = f\"{item['title']}\\n\\n{item['summary']}\"\n",
    "        transformed = transform_snippet(snippet)\n",
    "        \n",
    "        def extract_fields(text):\n",
    "            text = re.sub(r'\\*+', '', text)\n",
    "            fields = {\n",
    "                \"Headline\": re.search(r'Headline:\\s*(.*?)\\n', text).group(1).strip() if re.search(r'Headline:', text) else \"\",\n",
    "                \"Category\": re.search(r'Category:\\s*(.*?)\\n', text).group(1).strip() if re.search(r'Category:', text) else \"\",\n",
    "                \"State\": re.search(r'State:\\s*(.*?)\\n', text).group(1).strip() if re.search(r'State:', text) else \"\",\n",
    "                \"Subheading\": re.search(r'Subheading:\\s*(.*?)\\n', text).group(1).strip() if re.search(r'Subheading:', text) else \"\",\n",
    "                \"Content\": re.search(r'Content:\\s*(.*)', text).group(1).strip() if re.search(r'Content:', text) else \"\",\n",
    "                \"Source\": item['source'],\n",
    "                \"Published_Time\": item['published'],\n",
    "                \"Image_URL\": item['image']\n",
    "            }\n",
    "            return fields\n",
    "        \n",
    "        processed_news.append(extract_fields(transformed))\n",
    "    \n",
    "    return pd.DataFrame(processed_news)\n",
    "\n",
    "# Image Processing Functions\n",
    "def mm_to_pixels(mm, dpi=600):\n",
    "    return int(mm * dpi / 25.4)\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGBA\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from URL: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_edge_transparency(img, fade_percent=20):\n",
    "    width, height = img.size\n",
    "    fade_width = int(width * fade_percent / 100)\n",
    "    fade_height = int(height * fade_percent / 100)\n",
    "    \n",
    "    alpha = Image.new('L', (width, height), 255)\n",
    "    draw = ImageDraw.Draw(alpha)\n",
    "    \n",
    "    for x in range(fade_width):\n",
    "        opacity = int(255 * (x / fade_width))\n",
    "        draw.line([(x, 0), (x, height)], fill=opacity)\n",
    "    \n",
    "    for x in range(width - fade_width, width):\n",
    "        opacity = int(255 * ((width - x) / fade_width))\n",
    "        draw.line([(x, 0), (x, height)], fill=opacity)\n",
    "    \n",
    "    for y in range(fade_height):\n",
    "        opacity = int(255 * (y / fade_height))\n",
    "        draw.line([(0, y), (width, y)], fill=opacity)\n",
    "    \n",
    "    for y in range(height - fade_height, height):\n",
    "        opacity = int(255 * ((height - y) / fade_height))\n",
    "        draw.line([(0, y), (width, y)], fill=opacity)\n",
    "    \n",
    "    img.putalpha(alpha)\n",
    "    return img\n",
    "\n",
    "def add_bottom_gradient(img, fade_height_percent=50):\n",
    "    width, height = img.size\n",
    "    fade_height = int(height * fade_height_percent / 100)\n",
    "    \n",
    "    gradient = Image.new('L', (width, fade_height))\n",
    "    for y in range(fade_height):\n",
    "        alpha = int(255 * (1.2 * y / fade_height)**2)\n",
    "        gradient.paste(alpha, (0, y, width, y+1))\n",
    "    \n",
    "    black_layer = Image.new('RGB', (width, fade_height), (0, 0, 0))\n",
    "    black_layer.putalpha(gradient)\n",
    "    \n",
    "    result = img.convert('RGBA')\n",
    "    result.paste(black_layer, (0, height - fade_height), black_layer)\n",
    "    return result\n",
    "\n",
    "def create_instagram_post(template_path, headline, subheading, subsubheading, \n",
    "                         logo_path=None, image_url=None, source=\"\", publish_date=\"\", category=\"\", state=\"\"):\n",
    "    \"\"\"Create and display Instagram post without saving to file.\"\"\"\n",
    "    img = Image.open(template_path).convert(\"RGB\")\n",
    "    width, height = img.size\n",
    "    \n",
    "    if image_url:\n",
    "        url_image = load_image_from_url(image_url)\n",
    "        if url_image:\n",
    "            side_margin = mm_to_pixels(10)\n",
    "            max_image_width = width - 2 * side_margin\n",
    "            \n",
    "            url_img_width, url_img_height = url_image.size\n",
    "            scale_factor = min(2.5, max_image_width / url_img_width)\n",
    "            new_width = int(url_img_width * scale_factor)\n",
    "            new_height = int(url_img_height * scale_factor)\n",
    "            \n",
    "            url_image = url_image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "            url_image = add_edge_transparency(url_image)\n",
    "            \n",
    "            x_position = side_margin + (max_image_width - new_width) // 2\n",
    "            y_position = (height - new_height) // 2\n",
    "            \n",
    "            url_layer = Image.new('RGBA', img.size, (0, 0, 0, 0))\n",
    "            url_layer.paste(url_image, (x_position, y_position))\n",
    "            \n",
    "            img = Image.alpha_composite(img.convert('RGBA'), url_layer)\n",
    "    \n",
    "    img = add_bottom_gradient(img, 40)\n",
    "    \n",
    "    logo_margin = 5\n",
    "    if logo_path:\n",
    "        try:\n",
    "            logo = Image.open(logo_path).convert(\"RGBA\")\n",
    "            logo_size = mm_to_pixels(15)\n",
    "            logo_height = int(logo_size * (logo.size[1] / logo.size[0]))\n",
    "            logo = logo.resize((logo_size, logo_height), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            logo_layer = Image.new('RGBA', img.size, (0, 0, 0, 0))\n",
    "            logo_layer.paste(logo, (mm_to_pixels(logo_margin), mm_to_pixels(logo_margin)), logo)\n",
    "            \n",
    "            img = Image.alpha_composite(img, logo_layer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading logo: {e}\")\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    State_Size = 65\n",
    "    Category_Size = 120\n",
    "    Head_Size = 80\n",
    "    SbHead_Size = 75\n",
    "    B_Size = 50\n",
    "    Source_Size = 40\n",
    "    \n",
    "    state_color = \"white\"\n",
    "    category_color = \"white\"\n",
    "    headline_color = \"white\"\n",
    "    subheading_color = 'red'\n",
    "    subsubheading_color = \"white\" \n",
    "    source_color = (200, 200, 200)\n",
    "    box_fill = (112, 128, 144, 0)\n",
    "    box_outline = \"#000000\"\n",
    "    box_outline_width = mm_to_pixels(0.08)\n",
    "    text_stroke_width = 2\n",
    "\n",
    "    category_headline_space = 30\n",
    "    headline_subheading_space = 60\n",
    "    subheading_subsubheading_space = 40\n",
    "    line_spacing = 2.5\n",
    "    margin_px = 100\n",
    "    max_text_width = width - (1 + 2 * margin_px)\n",
    "    radius_px = mm_to_pixels(1.5)\n",
    "    box_margin = 25\n",
    "    source_margin = 20\n",
    "\n",
    "    def load_font(font_paths, size):\n",
    "        for path in font_paths:\n",
    "            try:\n",
    "                return ImageFont.truetype(path, size)\n",
    "            except:\n",
    "                continue\n",
    "        return ImageFont.load_default(size)\n",
    "\n",
    "    font_paths = [\n",
    "        \"times.ttf\",\n",
    "        \"arial.ttf\",\n",
    "        \"arialbd.ttf\"\n",
    "    ]\n",
    "    \n",
    "    state_font = load_font(font_paths, State_Size)\n",
    "    category_font = load_font(font_paths, Category_Size)\n",
    "    headline_font = load_font(font_paths, Head_Size)\n",
    "    subhead_font = load_font(font_paths, SbHead_Size)\n",
    "    body_font = load_font(font_paths, B_Size)\n",
    "    source_font = load_font(font_paths, Source_Size)\n",
    "\n",
    "    if state:\n",
    "        state_text = f\"{state.capitalize()}\"\n",
    "        state_x = mm_to_pixels(logo_margin) + 170\n",
    "        state_y = mm_to_pixels(logo_margin) + mm_to_pixels(20) - 125\n",
    "        \n",
    "        draw.text(\n",
    "            (state_x, state_y),\n",
    "            state_text,\n",
    "            font=state_font,\n",
    "            fill=state_color,\n",
    "            anchor=\"mm\"\n",
    "        )\n",
    "\n",
    "    def get_text_height(text, font):\n",
    "        if not text or pd.isna(text):\n",
    "            return 0\n",
    "        lines = textwrap.wrap(str(text), width=int(max_text_width/(font.size*0.4)))\n",
    "        return (font.size + line_spacing) * len(lines)\n",
    "\n",
    "    category_lines = textwrap.wrap(category.upper(), width=int(max_text_width/(Category_Size*0.7)))\n",
    "    category_height = (Category_Size + line_spacing) * len(category_lines)\n",
    "    headline_lines = textwrap.wrap(headline.upper(), width=int(max_text_width/(Head_Size*0.7)))\n",
    "    headline_height = (Head_Size + line_spacing) * len(headline_lines)\n",
    "    subhead_height = get_text_height(subheading, subhead_font)\n",
    "    subsub_lines = textwrap.wrap(subsubheading, width=int(max_text_width/(B_Size*0.5)))\n",
    "    subsub_height = (B_Size + line_spacing) * len(subsub_lines)\n",
    "\n",
    "    min_content_space = (category_height + category_headline_space +\n",
    "                        headline_height + headline_subheading_space + \n",
    "                        subhead_height + subheading_subsubheading_space + \n",
    "                        subsub_height + 2*box_margin)\n",
    "    \n",
    "    gradient_top = height - int(height * 0.4)\n",
    "    available_space = gradient_top - margin_px\n",
    "    \n",
    "    if min_content_space > available_space:\n",
    "        scale_factor = available_space / min_content_space\n",
    "        Category_Size = int(Category_Size * scale_factor * 0.9)\n",
    "        Head_Size = int(Head_Size * scale_factor * 0.9)\n",
    "        SbHead_Size = int(SbHead_Size * scale_factor * 0.9)\n",
    "        B_Size = int(B_Size * scale_factor * 0.9)\n",
    "        \n",
    "        category_font = load_font(font_paths, Category_Size)\n",
    "        headline_font = load_font(font_paths, Head_Size)\n",
    "        subhead_font = load_font(font_paths, SbHead_Size)\n",
    "        body_font = load_font(font_paths, B_Size)\n",
    "        \n",
    "        category_lines = textwrap.wrap(category.upper(), width=int(max_text_width/(Category_Size*0.7)))\n",
    "        category_height = (Category_Size + line_spacing) * len(category_lines)\n",
    "        headline_lines = textwrap.wrap(headline.upper(), width=int(max_text_width/(Head_Size*0.7)))\n",
    "        headline_height = (Head_Size + line_spacing) * len(headline_lines)\n",
    "        subhead_height = get_text_height(str(subheading), subhead_font) if subheading else 0\n",
    "        subsub_lines = textwrap.wrap(str(subsubheading), width=int(max_text_width/(B_Size*0.5))) if subsubheading else []\n",
    "        subsub_height = (B_Size + line_spacing) * len(subsub_lines)\n",
    "\n",
    "    current_y = height - margin_px - (category_height + category_headline_space +\n",
    "                                    headline_height + headline_subheading_space + \n",
    "                                    subhead_height + subheading_subsubheading_space + \n",
    "                                    subsub_height + 2*box_margin)\n",
    "    \n",
    "    min_y = mm_to_pixels(logo_margin) + (mm_to_pixels(25) if logo_path else 0) + 20\n",
    "    current_y = max(current_y, min_y)\n",
    "\n",
    "    box_x1 = margin_px - box_margin\n",
    "    box_x2 = width - margin_px + box_margin\n",
    "    box_y1 = current_y + category_height + category_headline_space + headline_height + headline_subheading_space + subhead_height + subheading_subsubheading_space - box_margin\n",
    "    box_y2 = box_y1 + subsub_height + 2 * box_margin\n",
    "    box_y2 = min(box_y2, height - 10)\n",
    "\n",
    "    box_layer = Image.new('RGBA', img.size, (0, 0, 0, 0))\n",
    "    box_draw = ImageDraw.Draw(box_layer)\n",
    "    box_draw.rounded_rectangle(\n",
    "        [box_x1, box_y1, box_x2, box_y2],\n",
    "        radius=radius_px,\n",
    "        fill=box_fill,\n",
    "        outline=box_outline,\n",
    "        width=box_outline_width\n",
    "    )\n",
    "    img = Image.alpha_composite(img, box_layer)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    def draw_text_with_stroke(x, y, text, font, fill, stroke_fill=\"black\", stroke_width=2):\n",
    "        anchor = 'lm'\n",
    "        for dx in [-stroke_width, stroke_width]:\n",
    "            for dy in [-stroke_width, stroke_width]:\n",
    "                draw.text((x + dx, y + dy), text, font=font, fill=stroke_fill, anchor=anchor)\n",
    "        draw.text((x, y), text, font=font, fill=fill, anchor=anchor)\n",
    "\n",
    "    for line in category_lines:\n",
    "        text_width = category_font.getlength(line)\n",
    "        text_height = Category_Size\n",
    "        \n",
    "        bg_y1 = current_y - (text_height // 3) - 11\n",
    "        bg_y2 = current_y + (text_height * 2) // 3 - 11\n",
    "        bg_x1 = margin_px - 10\n",
    "        bg_x2 = margin_px + text_width + 10\n",
    "        \n",
    "        draw.rectangle(\n",
    "            [bg_x1, bg_y1, bg_x2, bg_y2],\n",
    "            fill=\"lime\",\n",
    "            outline=None\n",
    "        )\n",
    "        \n",
    "        draw_text_with_stroke(\n",
    "            margin_px, current_y, line,\n",
    "            font=category_font,\n",
    "            fill=category_color,\n",
    "            stroke_width=text_stroke_width\n",
    "        )\n",
    "        current_y += Category_Size + line_spacing\n",
    "    \n",
    "    current_y += category_headline_space\n",
    "\n",
    "    for line in headline_lines:\n",
    "        draw_text_with_stroke(\n",
    "            margin_px, current_y, line,\n",
    "            font=headline_font,\n",
    "            fill=headline_color,\n",
    "            stroke_width=text_stroke_width\n",
    "        )\n",
    "        current_y += Head_Size + line_spacing\n",
    "    \n",
    "    current_y += headline_subheading_space - line_spacing\n",
    "    \n",
    "    for line in textwrap.wrap(subheading, width=int(max_text_width/(SbHead_Size*0.5))):\n",
    "        draw_text_with_stroke(\n",
    "            margin_px, current_y, line,\n",
    "            font=subhead_font,\n",
    "            fill=subheading_color,\n",
    "            stroke_width=text_stroke_width\n",
    "        )\n",
    "        current_y += SbHead_Size + line_spacing\n",
    "    \n",
    "    current_y += subheading_subsubheading_space\n",
    "    \n",
    "    def draw_justified(text, font, start_y, color, max_height, original_font_size):\n",
    "        y = start_y\n",
    "        current_font_size = original_font_size\n",
    "        font = load_font(font_paths, current_font_size)\n",
    "        line_height = current_font_size + line_spacing\n",
    "        lines = []\n",
    "        min_font_size = int(original_font_size * 0.6)\n",
    "    \n",
    "        wrapped_lines = textwrap.wrap(text, width=int(max_text_width / (current_font_size * 0.5)))\n",
    "        required_height = len(wrapped_lines) * line_height\n",
    "    \n",
    "        if required_height <= max_height:\n",
    "            lines = wrapped_lines\n",
    "        else:\n",
    "            while current_font_size >= min_font_size and required_height > max_height:\n",
    "                current_font_size -= 1\n",
    "                font = load_font(font_paths, current_font_size)\n",
    "                line_height = current_font_size + line_spacing\n",
    "                wrapped_lines = textwrap.wrap(text, width=int(max_text_width / (current_font_size * 0.5)))\n",
    "                required_height = len(wrapped_lines) * line_height\n",
    "    \n",
    "            lines = wrapped_lines\n",
    "    \n",
    "        if required_height > max_height and lines:\n",
    "            lines = wrapped_lines[:max_height // line_height]\n",
    "            if lines:\n",
    "                lines[-1] = lines[-1][:max(0, len(lines[-1])-3)] + \"...\"\n",
    "    \n",
    "        y = start_y\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                total_width = sum(font.getlength(word) for word in words)\n",
    "                space_width = (max_text_width - total_width) / (len(words) - 1)\n",
    "                x = margin_px\n",
    "                for word in words[:-1]:\n",
    "                    draw.text((x, y), word, font=font, fill=color, anchor='lm')\n",
    "                    x += font.getlength(word) + space_width\n",
    "                draw.text((x, y), words[-1], font=font, fill=color, anchor='lm')\n",
    "            else:\n",
    "                draw.text((margin_px, y), line, font=font, fill=color, anchor='lm')\n",
    "            y += line_height\n",
    "    \n",
    "    draw_justified(subsubheading, body_font, current_y, subsubheading_color, \n",
    "                  box_y2 - current_y, B_Size)\n",
    "\n",
    "    if source or publish_date:\n",
    "        source_text = f\"Source: {source}\" if source else \"\"\n",
    "        date_text = f\"Published: {publish_date}\" if publish_date else \"\"\n",
    "        info_text = \" | \".join(filter(None, [source_text, date_text]))\n",
    "        \n",
    "        text_width = source_font.getlength(info_text)\n",
    "        text_x = width - margin_px - text_width\n",
    "        text_y = height - margin_px // 2\n",
    "        \n",
    "        draw.text(\n",
    "            (text_x, text_y),\n",
    "            info_text,\n",
    "            font=source_font,\n",
    "            fill=source_color,\n",
    "            anchor=\"lm\"\n",
    "        )\n",
    "\n",
    "    img.show()\n",
    "    img.save(f\"E:\\\\Intapost_Templates\\\\Download\\\\{random.randint(1, 50)}.png\")\n",
    "\n",
    "def generate_images_from_news():\n",
    "    \"\"\"Main function to scrape news, process it, and generate images.\"\"\"\n",
    "    scraper = NewsScraper()\n",
    "    news_items = scraper.get_all_news(limit_per_source=2)  # Get 2 articles per source\n",
    "    \n",
    "    if not news_items:\n",
    "        print(\"No news articles found.\")\n",
    "        return\n",
    "    \n",
    "    processed_df = process_news_data(news_items)\n",
    "    \n",
    "    for i in range(processed_df.shape[0]):\n",
    "        headline = str(processed_df['Headline'].iloc[i]) if pd.notna(processed_df['Headline'].iloc[i]) else \"\"\n",
    "        subheading = str(processed_df['Subheading'].iloc[i]) if pd.notna(processed_df['Subheading'].iloc[i]) else \"\"\n",
    "        subsubheading = str(processed_df['Content'].iloc[i]) if pd.notna(processed_df['Content'].iloc[i]) else \"\"\n",
    "        source = str(processed_df['Source'].iloc[i]) if pd.notna(processed_df['Source'].iloc[i]) else \"\"\n",
    "        publish_date = str(processed_df['Published_Time'].iloc[i]) if pd.notna(processed_df['Published_Time'].iloc[i]) else \"\"\n",
    "        category = str(processed_df['Category'].iloc[i]) if pd.notna(processed_df['Category'].iloc[i]) else \"\"\n",
    "        state = str(processed_df['State'].iloc[i]) if pd.notna(processed_df['State'].iloc[i]) else \"\"\n",
    "        \n",
    "        # Check if URL is valid\n",
    "        image_url = processed_df['Image_URL'].iloc[i]\n",
    "        is_valid_url = False\n",
    "        if pd.notna(image_url):\n",
    "            try:\n",
    "                parsed = urlparse(str(image_url))\n",
    "                is_valid_url = all([parsed.scheme in ['http','https'], parsed.netloc])\n",
    "            except:\n",
    "                is_valid_url = False\n",
    "        \n",
    "        # Generate random template path\n",
    "        template_path = f\"{TEMPLATE_FOLDER}{random.randint(1, 6)}.png\"\n",
    "        \n",
    "        print(f\"\\nGenerating image for news item {i+1}: {headline[:50]}...\")\n",
    "        \n",
    "        # Create and display Instagram post\n",
    "        # create_instagram_post(\n",
    "        #     image_path=template_path,\n",
    "        #     category=category,\n",
    "        #     headline=headline,\n",
    "        #     subheading=subheading,\n",
    "        #     subsubheading=subsubheading,\n",
    "        #     logo_path=LOGO_PATH,\n",
    "        #     image_url=image_url if is_valid_url else None,\n",
    "        #     source=source,\n",
    "        #     publish_date=publish_date,\n",
    "        #     state=state\n",
    "        # )\n",
    "\n",
    "        create_instagram_post(\n",
    "            template_path=template_path,  # Changed from image_path to template_path\n",
    "            category=category,\n",
    "            headline=headline,\n",
    "            subheading=subheading,\n",
    "            subsubheading=subsubheading,\n",
    "            logo_path=LOGO_PATH,\n",
    "            image_url=image_url if is_valid_url else None,\n",
    "            source=source,\n",
    "            publish_date=publish_date,\n",
    "            state=state\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# API_KEY =  API_Key#\"YOUR_API_KEY\"  # Replace with your actual API key\n",
    "# TEMPLATE_FOLDER = r\"E:\\Intapost_Templates\\Square Templetes 2\"\n",
    "# LOGO_PATH = r\"E:\\Intapost_Templates\\Square Templetes 2\\9.png\"\n",
    "# \"E:\\Intapost_Templates\\Square Templetes 2\\0.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fec777-be82-477f-8c46-ce51c409e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    generate_images_from_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39f07b4-3039-42e4-98d0-77d4635e8f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“° News Scraper with Image Generator\n",
      "Type 'start' to begin scraping and 'stop' to end\n",
      "Images will be generated every minute while running\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter command (start/stop/exit):  start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ News scraping started! Generating images every minute...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 06:34:53,394 - INFO - Fetching Hindustan Times RSS feed\n",
      "2025-07-16 06:34:54,375 - INFO - Fetching The Hindu RSS feed\n",
      "2025-07-16 06:34:56,646 - INFO - Fetching Indian Express RSS feed\n",
      "2025-07-16 06:34:59,912 - INFO - Fetching BBC RSS feed\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter command (start/stop/exit):  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ›‘ Scraping stopped. No new images will be generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 06:35:03,852 - INFO - Fetching CNN RSS feed\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter command (start/stop/exit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ›‘ Scraping stopped. No new images will be generated.\n",
      "ðŸ‘‹ Exiting program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 06:35:10,961 - INFO - Fetching Reuters RSS feed\n",
      "2025-07-16 06:35:12,785 - INFO - Fetching Al Jazeera RSS feed\n",
      "2025-07-16 06:35:15,311 - INFO - Fetching Money Control RSS feed\n",
      "2025-07-16 06:35:19,291 - ERROR - Request failed for https://www.moneycontrol.com/news/recommendations/buy-hdfc-bank-targetrs-1850-icici-securities_17531671.html: 403 Client Error: Forbidden for url: https://www.moneycontrol.com/news/recommendations/buy-hdfc-bank-targetrs-1850-icici-securities_17531671.html\n",
      "2025-07-16 06:35:27,069 - ERROR - Request failed for https://www.moneycontrol.com/news/recommendations/buy-tejas-networks-targetrs-1100-emkay-global-financial_17531621.html: 403 Client Error: Forbidden for url: https://www.moneycontrol.com/news/recommendations/buy-tejas-networks-targetrs-1100-emkay-global-financial_17531621.html\n",
      "2025-07-16 06:35:31,575 - INFO - Fetching News18 RSS feed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“° Generated image for: Jaishankar Condemns Terrorism at SCO Meeting; UIDA...\n",
      "\n",
      "ðŸ“° Generated image for: Kamaraj's Electoral Triumphs in Tamil Nadu Bye-Ele...\n",
      "\n",
      "ðŸ“° Generated image for: Bombay HC Rejects Plea to Reopen Mumbai's Kabootar...\n",
      "\n",
      "ðŸ“° Generated image for: Rajasthan Court Suspends Sentences of Congress MLA...\n",
      "\n",
      "ðŸ“° Generated image for: MasterChef Host John Torode Sacked After Racist Re...\n",
      "\n",
      "ðŸ“° Generated image for: UK's Secret Afghan Resettlement: 4,500 Relocated A...\n",
      "\n",
      "ðŸ“° Generated image for: NH Mom Wins License Plate Battle: \"PB4WEGO\"...\n",
      "\n",
      "ðŸ“° Generated image for: Russia-Ukraine War: Day 1238 Key Events...\n",
      "\n",
      "ðŸ“° Generated image for: AI-Fueled Disinformation Intensifies Philippine Po...\n",
      "\n",
      "ðŸ“° Generated image for: ICICI Securities Recommends HDFC Bank Purchase; Rs...\n",
      "\n",
      "ðŸ“° Generated image for: Emkay Global Recommends Tejas Networks Buy, Target...\n",
      "\n",
      "ðŸ“° Generated image for: Supreme Court Upholds Life Sentences in 2003 Benga...\n",
      "\n",
      "ðŸ“° Generated image for: Maharashtra Couple Arrested for Throwing Newborn f...\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "# Add this class to manage the scraping process\n",
    "class ScrapingController:\n",
    "    def __init__(self):\n",
    "        self.should_run = False\n",
    "        self.scraping_queue = Queue()\n",
    "        self.scraper = NewsScraper()\n",
    "    \n",
    "    def start_scraping(self):\n",
    "        self.should_run = True\n",
    "        print(\"\\nðŸš€ News scraping started! Generating images every minute...\")\n",
    "        threading.Thread(target=self._continuous_scraping, daemon=True).start()\n",
    "    \n",
    "    def stop_scraping(self):\n",
    "        self.should_run = False\n",
    "        print(\"\\nðŸ›‘ Scraping stopped. No new images will be generated.\")\n",
    "    \n",
    "    def _continuous_scraping(self):\n",
    "        while self.should_run:\n",
    "            try:\n",
    "                news_items = self.scraper.get_all_news(limit_per_source=2)\n",
    "                if news_items:\n",
    "                    processed_df = process_news_data(news_items)\n",
    "                    self._generate_images(processed_df)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error during scraping: {e}\")\n",
    "            \n",
    "            # Wait for 1 minute before next scrape\n",
    "            for _ in range(60):\n",
    "                if not self.should_run:\n",
    "                    return\n",
    "                time.sleep(1)\n",
    "    \n",
    "    def _generate_images(self, processed_df):\n",
    "        for i in range(processed_df.shape[0]):\n",
    "            headline = str(processed_df['Headline'].iloc[i]) if pd.notna(processed_df['Headline'].iloc[i]) else \"\"\n",
    "            subheading = str(processed_df['Subheading'].iloc[i]) if pd.notna(processed_df['Subheading'].iloc[i]) else \"\"\n",
    "            subsubheading = str(processed_df['Content'].iloc[i]) if pd.notna(processed_df['Content'].iloc[i]) else \"\"\n",
    "            source = str(processed_df['Source'].iloc[i]) if pd.notna(processed_df['Source'].iloc[i]) else \"\"\n",
    "            publish_date = str(processed_df['Published_Time'].iloc[i]) if pd.notna(processed_df['Published_Time'].iloc[i]) else \"\"\n",
    "            category = str(processed_df['Category'].iloc[i]) if pd.notna(processed_df['Category'].iloc[i]) else \"\"\n",
    "            state = str(processed_df['State'].iloc[i]) if pd.notna(processed_df['State'].iloc[i]) else \"\"\n",
    "            \n",
    "            image_url = processed_df['Image_URL'].iloc[i]\n",
    "            is_valid_url = False\n",
    "            if pd.notna(image_url):\n",
    "                try:\n",
    "                    parsed = urlparse(str(image_url))\n",
    "                    is_valid_url = all([parsed.scheme in ['http','https'], parsed.netloc])\n",
    "                except:\n",
    "                    is_valid_url = False\n",
    "            \n",
    "            template_path = f\"{TEMPLATE_FOLDER}{random.randint(1, 6)}.png\"\n",
    "            \n",
    "            print(f\"\\nðŸ“° Generated image for: {headline[:50]}...\")\n",
    "            \n",
    "            create_instagram_post(\n",
    "                template_path=template_path,\n",
    "                category=category,\n",
    "                headline=headline,\n",
    "                subheading=subheading,\n",
    "                subsubheading=subsubheading,\n",
    "                logo_path=LOGO_PATH,\n",
    "                image_url=image_url if is_valid_url else None,\n",
    "                source=source,\n",
    "                publish_date=publish_date,\n",
    "                state=state\n",
    "            )\n",
    "\n",
    "# Replace the if __name__ == \"__main__\" block with this:\n",
    "if __name__ == \"__main__\":\n",
    "    controller = ScrapingController()\n",
    "    \n",
    "    print(\"ðŸ“° News Scraper with Image Generator\")\n",
    "    print(\"Type 'start' to begin scraping and 'stop' to end\")\n",
    "    print(\"Images will be generated every minute while running\\n\")\n",
    "    \n",
    "    while True:\n",
    "        command = input(\"Enter command (start/stop/exit): \").strip().lower()\n",
    "        \n",
    "        if command == \"start\":\n",
    "            if not controller.should_run:\n",
    "                controller.start_scraping()\n",
    "            else:\n",
    "                print(\"Scraping is already running!\")\n",
    "        \n",
    "        elif command == \"stop\":\n",
    "            if controller.should_run:\n",
    "                controller.stop_scraping()\n",
    "            else:\n",
    "                print(\"Scraping isn't currently running\")\n",
    "        \n",
    "        elif command == \"exit\":\n",
    "            controller.stop_scraping()\n",
    "            print(\"ðŸ‘‹ Exiting program...\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid command. Please use 'start', 'stop', or 'exit'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a526d3-f60e-429b-8891-2d071582ed72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import tracemalloc\n",
    "\n",
    "class MemorySafeScraper:\n",
    "    def __init__(self):\n",
    "        self.is_running = False\n",
    "        self.image_counter = 0\n",
    "        self.max_images = 10  # Limit before cleanup\n",
    "        tracemalloc.start()\n",
    "        \n",
    "    async def generate_safe_images(self, df):\n",
    "        \"\"\"Memory-optimized image generation pipeline\"\"\"\n",
    "        for i in range(len(df)):\n",
    "            if not self.is_running:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Process data first\n",
    "                row = df.iloc[i]\n",
    "                template_path = f\"{TEMPLATE_FOLDER}{random.randint(1, 6)}.png\"\n",
    "                \n",
    "                # Create image with manual memory control\n",
    "                img = await self._create_image_with_cleanup(\n",
    "                    template_path=template_path,\n",
    "                    category=str(row['Category']),\n",
    "                    headline=str(row['Headline']),\n",
    "                    subheading=str(row['Subheading']),\n",
    "                    subsubheading=str(row['Content']),\n",
    "                    logo_path=LOGO_PATH,\n",
    "                    image_url=str(row['Image_URL']) if pd.notna(row['Image_URL']) else None,\n",
    "                    source=str(row['Source']),\n",
    "                    publish_date=str(row['Published_Time']),\n",
    "                    state=str(row['State'])\n",
    "                )\n",
    "                \n",
    "                # Controlled display\n",
    "                await self._safe_display(img)\n",
    "                self.image_counter += 1\n",
    "                \n",
    "                # Proactive cleanup\n",
    "                if self.image_counter % 3 == 0:\n",
    "                    await self._deep_cleanup()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error on item {i}: {str(e)[:100]}...\")\n",
    "                await self._emergency_cleanup()\n",
    "\n",
    "    async def _create_image_with_cleanup(self, **kwargs):\n",
    "        \"\"\"Wrapper with memory safeguards\"\"\"\n",
    "        try:\n",
    "            # Create in separate thread with memory limit\n",
    "            return await asyncio.to_thread(\n",
    "                self._create_image, \n",
    "                **kwargs\n",
    "            )\n",
    "        finally:\n",
    "            # Immediate cleanup of temporary objects\n",
    "            gc.collect()\n",
    "            \n",
    "    def _create_image(self, **kwargs):\n",
    "        \"\"\"Original image creation with pixel buffer management\"\"\"\n",
    "        # Convert template to numpy array for better memory control\n",
    "        with Image.open(kwargs['template_path']) as template:\n",
    "            img_array = np.array(template)\n",
    "            \n",
    "        # Process image through all original steps\n",
    "        img = create_instagram_post(**kwargs)\n",
    "        \n",
    "        # Convert back to PIL Image and downscale if too large\n",
    "        if img.size[0] * img.size[1] > 2000*2000:  # If over 4MP\n",
    "            img = img.resize((int(img.size[0]*0.8), (int(img.size[1]*0.8)), \n",
    "                          Image.Resampling.LANCZOS) ) \n",
    "        \n",
    "        return img\n",
    "\n",
    "    async def _safe_display(self, img):\n",
    "        \"\"\"Memory-efficient display\"\"\"\n",
    "        try:\n",
    "            # Convert to JPEG to reduce memory footprint\n",
    "            with BytesIO() as buffer:\n",
    "                img.save(buffer, format='JPEG', quality=85)\n",
    "                buffer.seek(0)\n",
    "                display_img = Image.open(buffer)\n",
    "                \n",
    "            clear_output(wait=True)\n",
    "            display(display_img)\n",
    "            print(f\"ðŸ“° Displayed image {self.image_counter}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ðŸš¨ Display error: {str(e)[:100]}...\")\n",
    "        finally:\n",
    "            del img\n",
    "            gc.collect()\n",
    "\n",
    "    async def _deep_cleanup(self):\n",
    "        \"\"\"Aggressive memory cleanup\"\"\"\n",
    "        print(\"ðŸ§¹ Performing deep cleanup...\")\n",
    "        gc.collect()\n",
    "        # Clear matplotlib and IPython outputs\n",
    "        plt.close('all')\n",
    "        clear_output(wait=False)\n",
    "        \n",
    "        # Check memory usage\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        print(f\"Memory: {current/10**6:.1f}MB (Peak: {peak/10**6:.1f}MB)\")\n",
    "        \n",
    "        # Reset counter periodically\n",
    "        if self.image_counter >= self.max_images:\n",
    "            print(\"â™»ï¸ Resetting image counter\")\n",
    "            self.image_counter = 0\n",
    "\n",
    "    async def _emergency_cleanup(self):\n",
    "        \"\"\"Critical memory recovery\"\"\"\n",
    "        print(\"ðŸš¨ EMERGENCY MEMORY CLEANUP\")\n",
    "        gc.collect()\n",
    "        clear_output(wait=False)\n",
    "        await asyncio.sleep(1)  # Let system recover\n",
    "\n",
    "# Modified control function\n",
    "def jupyter_control():\n",
    "    controller = MemorySafeScraper()\n",
    "    \n",
    "    async def start_scraping():\n",
    "        controller.is_running = True\n",
    "        scraper = NewsScraper()\n",
    "        while controller.is_running:\n",
    "            try:\n",
    "                news_items = await asyncio.to_thread(\n",
    "                    scraper.get_all_news, \n",
    "                    limit_per_source=1\n",
    "                )\n",
    "                if news_items:\n",
    "                    processed_df = await asyncio.to_thread(\n",
    "                        process_news_data, \n",
    "                        news_items\n",
    "                    )\n",
    "                    await controller.generate_safe_images(processed_df)\n",
    "                    \n",
    "                await asyncio.sleep(60)  # Throttle scraping\n",
    "            except Exception as e:\n",
    "                print(f\"ðŸ”´ Main error: {str(e)[:100]}...\")\n",
    "                await controller._emergency_cleanup()\n",
    "\n",
    "    def start(b):\n",
    "        asyncio.create_task(start_scraping())\n",
    "    \n",
    "    def stop(b):\n",
    "        controller.is_running = False\n",
    "        print(\"ðŸ›‘ Stopping safely...\")\n",
    "        asyncio.create_task(controller._deep_cleanup())\n",
    "\n",
    "    # Create Jupyter widgets\n",
    "    from ipywidgets import Button, HBox\n",
    "    start_btn = Button(description=\"Start\", button_style='success')\n",
    "    stop_btn = Button(description=\"Stop\", button_style='danger')\n",
    "    \n",
    "    start_btn.on_click(start)\n",
    "    stop_btn.on_click(stop)\n",
    "    \n",
    "    display(HBox([start_btn, stop_btn]))\n",
    "\n",
    "# Run this in Jupyter\n",
    "jupyter_control()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
